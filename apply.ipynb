{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Settings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjding/anaconda3/envs/PLM4ACE/lib/python3.8/site-packages/torch/cuda/__init__.py:88: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# use CUDA if possible\n",
    "import torch as th\n",
    "device = th.device('cuda:0' if th.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if th.cuda.is_available():\n",
    "    th.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_l_file = os.path.join(os.getcwd(), \"alternative_ACP_train.xlsx\")  # changeable\n",
    "train_d_file = os.path.join(os.getcwd(), \"train_embedded.csv\")\n",
    "test_l_file = os.path.join(os.getcwd(), \"alternative_ACP_test.xlsx\")  # changeable\n",
    "test_d_file = os.path.join(os.getcwd(), \"test_embedded.csv\")\n",
    "\n",
    "train_loss_log = os.path.join(os.getcwd(), \"train_log.txt\")\n",
    "test_loss_log = os.path.join(os.getcwd(), \"validation_log.txt\")\n",
    "checkpoint = os.path.join(os.getcwd(), 'bestmodel.pt')\n",
    "\n",
    "plot_range = 10000  # range(1, 10000)\n",
    "stratify = True\n",
    "batch_size = 16\n",
    "lr = 0.0001\n",
    "epochs = 500\n",
    "weight_decay = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data embedding\n",
    "\n",
    "** Do not change this part unless you know what you are doing\n",
    "\n",
    "Use ESM-2(t6_8M_UR50D) to embed the given sequences\n",
    "Reference: esm2_t6_8M_UR50D: https://github.com/facebookresearch/esm\n",
    "The raw data(with sequence and group) is read from l_file;\n",
    "and the embedded data will be stored in d_file\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import esm\n",
    "import collections\n",
    "\n",
    "def _esm2_embeddings(peptide_sequence_list, path: str) -> pd.DataFrame:\n",
    "    # NOTICE: ESM is RAM consuming, if your sequence is too long,\n",
    "    #         or you have too many sequences for transformation in a single converting;\n",
    "    #         your computer might automatically kill the job.\n",
    "\n",
    "    # load the model\n",
    "    # NOTICE: if the model was not downloaded in your local environment, it will automatically download it.\n",
    "    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()  # disables dropout for deterministic results\n",
    "\n",
    "    # load the peptide sequence list into the bach_converter\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(peptide_sequence_list)\n",
    "    batch_lens = (batch_tokens != alphabet.padding_idx).sum(1)\n",
    "    # batch tokens are the embedding results of the whole data set\n",
    "    # Extract per-residue representations (on CPU)\n",
    "    with torch.no_grad():\n",
    "        # Here we export the last layer of the EMS model output as the representation of the peptides\n",
    "        # model'esm2_t6_8M_UR50D' only has 6 layers, and therefore repr_layers parameters is equal to 6\n",
    "        results = model(batch_tokens, repr_layers=[6], return_contacts=True)\n",
    "    token_representations = results[\"representations\"][6]\n",
    "\n",
    "    # Generate per-sequence representations via averaging\n",
    "    # NOTE: token 0 is always a beginning-of-sequence token, so the first residue is token 1.\n",
    "    sequence_representations = []\n",
    "    for i, tokens_len in enumerate(batch_lens):\n",
    "        sequence_representations.append(token_representations[i, 1: tokens_len - 1].mean(0))\n",
    "    # save dataset\n",
    "    # sequence_representations is a list and each element is a tensor\n",
    "    embeddings_results = collections.defaultdict(list)\n",
    "    for i in range(len(sequence_representations)):\n",
    "        # tensor can be transformed as numpy sequence_representations[0].numpy() or sequence_representations[0].to_list\n",
    "        each_seq_rep = sequence_representations[i].tolist()\n",
    "        for each_element in each_seq_rep:\n",
    "            embeddings_results[i].append(each_element)\n",
    "    embeddings_results = pd.DataFrame(embeddings_results).T\n",
    "    embeddings_results.to_csv(path_or_buf=path, header=False, index=False, sep=' ')\n",
    "    return embeddings_results\n",
    "\n",
    "def get_embeddings(fdata: str, save_path: str):\n",
    "    sequences = pd.read_excel(fdata)['sequence'].to_numpy()\n",
    "    sequences = sequences.tolist()\n",
    "    sequences = [tuple([e, e]) for e in sequences]\n",
    "    res = _esm2_embeddings(sequences, save_path)\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "\"\"\"\n",
    "Read embedded data;\n",
    "split train and test set;\n",
    "normalization the data;\n",
    "assemble torch dataset and dataloader\n",
    "\"\"\"\n",
    "def get_raw_data(d_file: str, dimension:int, l_file: str):\n",
    "    \"\"\"\n",
    "    :param dimension: the dimension of the raw data : List\n",
    "    :param d_file: a csv file with embedded peptides\n",
    "    :param l_file: an Excel form with 'group' column\n",
    "    :return: raw dataset: ndarray, One-hot encoded labels: ndarray\n",
    "    \"\"\"\n",
    "    embedded_data = pd.read_csv(d_file, delimiter=' ', header=None, dtype=float, index_col=None)\n",
    "    embedded_data = embedded_data.to_numpy().reshape([-1, dimension]).astype(np.float32)\n",
    "    labels = pd.read_excel(l_file)['group'].to_numpy()\n",
    "    sequences = pd.read_excel(l_file)['sequence'].to_numpy()\n",
    "    return embedded_data, labels, sequences\n",
    "\n",
    "\n",
    "def label_mapper(label):\n",
    "    \"\"\"\n",
    "    Map group labels to One-hot encoded labels\n",
    "    :param label: be either \"A\", or \"B\" in the binary classification task\n",
    "    :return: A integer(either 1 or 0)\n",
    "    \"\"\"\n",
    "    return [1] if label == 0 else [0]\n",
    "\n",
    "\n",
    "def get_mapped_labels(data, labels,):\n",
    "    labels = np.array([label_mapper(x) for x in labels]).astype(np.float32)\n",
    "    if len(data) != len(labels):\n",
    "        raise ValueError(\"unmatched dataset\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_dataset_weight(labels: np.ndarray):\n",
    "    \"\"\"\n",
    "    get weights in case of imbalanced classes\n",
    "    :param labels: labels\n",
    "    :return: a vector of class weights: ndarray\n",
    "    \"\"\"\n",
    "    weight = compute_class_weight('balanced', classes=np.unique(labels), y=np.squeeze(labels))\n",
    "    return weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.data = th.from_numpy(x).float()\n",
    "        self.labels = th.from_numpy(y).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "\n",
    "def get_th_dataset(x, y):\n",
    "    \"\"\"\n",
    "    assemble a dataset with the given data and labels\n",
    "    :param x:\n",
    "    :param y:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataset = CustomDataset(x, y)\n",
    "    return _dataset\n",
    "\n",
    "\n",
    "def get_dataloader(dataset: Dataset, batch_size):\n",
    "    \"\"\"\n",
    "    assemble a dataloader with the given dataset\n",
    "    :param dataset:\n",
    "    :param batch_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _dataLoader = DataLoader(dataset=dataset, batch_size=batch_size, pin_memory=True,\n",
    "                             drop_last=True, shuffle=True)\n",
    "    return _dataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation Criteria"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, balanced_accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_confusion_matrix(y_pred: th.Tensor, y_test: th.Tensor):\n",
    "    \"\"\"\n",
    "    plot confusion matrix\n",
    "    :param y_pred: predictions\n",
    "    :param y_test: ground truth labels\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    predictions = th.argmax(y_pred, dim=-1).numpy()\n",
    "    labels = th.argmax(y_test, dim=-1).numpy()  # A:0, B:1, C:2, [D:3]\n",
    "    cm = confusion_matrix(labels, predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(labels))\n",
    "    disp.plot()\n",
    "    plt.show()\n",
    "    return cm\n",
    "\n",
    "\n",
    "\n",
    "def scores(y_pred: th.Tensor, y_test: th.Tensor):\n",
    "    predictions = th.argmax(y_pred, dim=-1).numpy()\n",
    "    labels = y_test.numpy()\n",
    "    # labels = th.argmax(y_test, dim=-1).numpy()\n",
    "    recall = recall_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    precision = precision_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    f1 = f1_score(y_pred=predictions, y_true=labels, average='binary')\n",
    "    accuracy = accuracy_score(y_pred=predictions, y_true=labels)\n",
    "    # auc_score = roc_auc_score(y_score=y_pred.detach().numpy(), y_true=y_test.detach().numpy())\n",
    "    corr = matthews_corrcoef(y_true=labels, y_pred=predictions)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true=labels, y_pred=predictions, )\n",
    "\n",
    "    report = {\n",
    "        \"recall\": recall,\n",
    "        \"precision\": precision,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        # \"auc\": auc_score,\n",
    "        'matthews_corrcoef': corr,\n",
    "        'balanced_accuracy': balanced_accuracy\n",
    "    }\n",
    "    return report\n",
    "\n",
    "\n",
    "def report(model: torch.nn.Module, dataset: CustomDataset):\n",
    "    _inputs, _labels = dataset.get_data(), dataset.get_labels()\n",
    "    print(_inputs.size(0))\n",
    "    predictions = model(_inputs)\n",
    "    res = scores(predictions, _labels.squeeze())\n",
    "    print('accuracy ' + str(res[\"accuracy\"]))\n",
    "    print('precision ' + str(res[\"precision\"]))\n",
    "    print('f1 ' + str(res[\"f1\"]))\n",
    "    print('recall ' + str(res[\"recall\"]))\n",
    "    # print('auc_score ' + str(res[\"auc\"]))\n",
    "    print('matthews_corrcoef ' + str(res[\"matthews_corrcoef\"]))\n",
    "    print('balanced_accuracy ' + str(res[\"balanced_accuracy\"]))\n",
    "    # get_confusion_matrix(predictions, _labels.squeeze())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=True):\n",
    "        super(AsymmetricLoss, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        loss = los_pos + los_neg\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            pt0 = xs_pos * y\n",
    "            pt1 = xs_neg * (1 - y)  # pt = p if t > 0 else 1-p\n",
    "            pt = pt0 + pt1\n",
    "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            loss *= one_sided_w\n",
    "\n",
    "        return -loss.sum()\n",
    "\n",
    "\n",
    "class AsymmetricLossOptimized(nn.Module):\n",
    "    ''' Notice - optimized version, minimizes memory allocation and gpu uploading,\n",
    "    favors inplace operations'''\n",
    "\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8, disable_torch_grad_focal_loss=False):\n",
    "        super(AsymmetricLossOptimized, self).__init__()\n",
    "\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.disable_torch_grad_focal_loss = disable_torch_grad_focal_loss\n",
    "        self.eps = eps\n",
    "\n",
    "        # prevent memory allocation and gpu uploading every iteration, and encourages inplace operations\n",
    "        self.targets = self.anti_targets = self.xs_pos = self.xs_neg = self.asymmetric_w = self.loss = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        \"\"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: input logits\n",
    "        y: targets (multi-label binarized vector)\n",
    "        \"\"\"\n",
    "\n",
    "        self.targets = y\n",
    "        self.anti_targets = 1 - y\n",
    "\n",
    "        # Calculating Probabilities\n",
    "        self.xs_pos = torch.sigmoid(x)\n",
    "        self.xs_neg = 1.0 - self.xs_pos\n",
    "\n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            self.xs_neg.add_(self.clip).clamp_(max=1)\n",
    "\n",
    "        # Basic CE calculation\n",
    "        self.loss = self.targets * torch.log(self.xs_pos.clamp(min=self.eps))\n",
    "        self.loss.add_(self.anti_targets * torch.log(self.xs_neg.clamp(min=self.eps)))\n",
    "\n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(False)\n",
    "            self.xs_pos = self.xs_pos * self.targets\n",
    "            self.xs_neg = self.xs_neg * self.anti_targets\n",
    "            self.asymmetric_w = torch.pow(1 - self.xs_pos - self.xs_neg,\n",
    "                                          self.gamma_pos * self.targets + self.gamma_neg * self.anti_targets)\n",
    "            if self.disable_torch_grad_focal_loss:\n",
    "                torch.set_grad_enabled(True)\n",
    "            self.loss *= self.asymmetric_w\n",
    "\n",
    "        return -self.loss.sum()\n",
    "\n",
    "\n",
    "class ASLSingleLabel(nn.Module):\n",
    "    '''\n",
    "    This loss is intended for single-label classification problems\n",
    "    '''\n",
    "    def __init__(self, gamma_pos=0, gamma_neg=4, eps: float = 0.1, reduction='mean'):\n",
    "        super(ASLSingleLabel, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "        self.targets_classes = []\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "        '''\n",
    "        \"input\" dimensions: - (batch_size,number_classes)\n",
    "        \"target\" dimensions: - (batch_size)\n",
    "        '''\n",
    "        num_classes = inputs.size()[-1]\n",
    "        log_preds = self.logsoftmax(inputs)\n",
    "        self.targets_classes = torch.zeros_like(inputs).scatter_(1, target.long().unsqueeze(1), 1)\n",
    "\n",
    "        # ASL weights\n",
    "        targets = self.targets_classes\n",
    "        anti_targets = 1 - targets\n",
    "        xs_pos = torch.exp(log_preds)\n",
    "        xs_neg = 1 - xs_pos\n",
    "        xs_pos = xs_pos * targets\n",
    "        xs_neg = xs_neg * anti_targets\n",
    "        asymmetric_w = torch.pow(1 - xs_pos - xs_neg,\n",
    "                                 self.gamma_pos * targets + self.gamma_neg * anti_targets)\n",
    "        log_preds = log_preds * asymmetric_w\n",
    "\n",
    "        if self.eps > 0:  # label smoothing\n",
    "            self.targets_classes = self.targets_classes.mul(1 - self.eps).add(self.eps / num_classes)\n",
    "\n",
    "        # loss calculation\n",
    "        loss = - self.targets_classes.mul(log_preds)\n",
    "\n",
    "        loss = loss.sum(dim=-1)\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv1d, Linear, Dropout, MaxPool1d, functional as F, BatchNorm1d, LazyLinear\n",
    "\n",
    "\n",
    "class Cnn(Module):\n",
    "    \"\"\"\n",
    "    CNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, output_dim=1, input_dim=320, drop_out=0, stride=2):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.drop_out = drop_out\n",
    "\n",
    "        self.kernel_1 = 3\n",
    "        self.channel_1 = 32\n",
    "\n",
    "        self.conv_1 = Conv1d(kernel_size=self.kernel_1, out_channels=self.channel_1, in_channels=1, stride=1)\n",
    "        self.normalizer_1 = BatchNorm1d(self.channel_1)\n",
    "        self.pooling_1 = MaxPool1d(kernel_size=self.kernel_1, stride=stride)\n",
    "\n",
    "        self.dropout = Dropout(p=drop_out)\n",
    "        self.fc1 = LazyLinear(64)\n",
    "        self.normalizer_2 = BatchNorm1d(64)\n",
    "        self.fc2 = Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = th.unsqueeze(x, dim=1)  # (batch, embedding_dim) -> (batch, 1, embedding_dim)\n",
    "        c_1 = self.pooling_1(F.relu(self.normalizer_1(self.conv_1(x))))\n",
    "\n",
    "        c_2 = th.flatten(c_1, start_dim=1)\n",
    "        c_2 = self.dropout(c_2)\n",
    "        out = F.relu(self.normalizer_2(self.fc1(c_2)))\n",
    "        out = self.fc2(out)\n",
    "        out = th.softmax(out, dim=-1)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from torch.nn import Module\n",
    "from torch.optim import Optimizer\n",
    "import os\n",
    "\n",
    "\n",
    "def to_log(epoch: int, loss: float, accuracy, logFile: str, is_append: bool):\n",
    "    info = str(epoch) + ' ' + str(loss) + ' ' + str(accuracy) + '\\n'\n",
    "    flag = 'a' if is_append else 'w'\n",
    "    file = open(logFile, flag)  # append mode\n",
    "    file.write(info)\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def train(model: Module, EPOCHS, optimizer: Optimizer, criteria,\n",
    "           checkpoint, train_set: DataLoader, vali_set: DataLoader, device, LOG_VALIDATION, LOG_TRAIN):\n",
    "    \"\"\"\n",
    "    fine tune the model and save the best model in the checkpoint\n",
    "    :param LOG_TRAIN:\n",
    "    :param LOG_VALIDATION:\n",
    "    :param device:\n",
    "    :param model: a Cnn or ConvLSTM model\n",
    "    :param EPOCHS: hyperparameter Epoch\n",
    "    :param optimizer: pytorch optimizer\n",
    "    :param criteria: loss function\n",
    "    :param checkpoint: model checkpoint\n",
    "    :param train_set: a dataloader\n",
    "    :param vali_set: a dataloader\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    if os.path.exists(LOG_VALIDATION):\n",
    "        os.remove(LOG_VALIDATION)\n",
    "    if os.path.exists(LOG_TRAIN):\n",
    "        os.remove(LOG_TRAIN)\n",
    "    model = model.to(device)\n",
    "    min_vali_loss = float(\"inf\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        train_acc = []\n",
    "        vali_loss = 0.0\n",
    "        model.train()\n",
    "        counter = 0\n",
    "        for i, (inputs, labels) in enumerate(train_set):\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # outputs = outputs.squeeze()\n",
    "            loss = criteria(outputs.float(), labels.float().squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            train_acc.append(scores(outputs.to(\"cpu\"), labels.to(\"cpu\"))[\"accuracy\"])\n",
    "            counter = i\n",
    "        model.eval()\n",
    "        acc = 0\n",
    "        for j, (vali_inputs, vali_labels) in enumerate(vali_set):\n",
    "            vali_labels = vali_labels.to(device)\n",
    "            vali_inputs = vali_inputs.to(device)\n",
    "            vali_outputs = model(vali_inputs)\n",
    "            # vali_outputs = vali_outputs.squeeze()\n",
    "            acc = scores(vali_outputs.to('cpu'), vali_labels.to('cpu'))[\"accuracy\"]\n",
    "            vali_loss = criteria(vali_outputs.to(device).float(), vali_labels.to(device).float().squeeze())\n",
    "            if vali_loss < min_vali_loss:\n",
    "                min_vali_loss = vali_loss\n",
    "                th.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                }, checkpoint)\n",
    "        avg_loss = running_loss / counter  # loss per batch\n",
    "        train_accuracy = sum(train_acc) / len(train_acc)\n",
    "        #print('epoch {} train_loss: {} vali_loss: {} test_acc: {}'\n",
    "        #      .format(epoch + 1, f'{avg_loss:5f}', f'{vali_loss:5f}', f'{acc: 5f}'))\n",
    "        # logs\n",
    "        to_log(epoch, avg_loss, train_accuracy, LOG_TRAIN, True)\n",
    "        to_log(epoch, vali_loss.item(), acc, LOG_VALIDATION, True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataloader assembling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "get_embeddings(fdata = train_l_file, save_path = train_d_file)\n",
    "get_embeddings(fdata = test_l_file, save_path = test_d_file)\n",
    "'''\n",
    "get raw data and one-hot encoded labels\n",
    "'''\n",
    "X_train, y_train, train_sequence = get_raw_data(d_file=train_d_file, dimension=320, l_file=train_l_file)\n",
    "X_test, y_test, test_sequence = get_raw_data(d_file=test_d_file, dimension=320, l_file=test_l_file)\n",
    "'''\n",
    "data normalization\n",
    "'''\n",
    "scalar = MinMaxScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.fit_transform(X_test)\n",
    "'''\n",
    "get one-hot encoded labels\n",
    "'''\n",
    "y_train = get_mapped_labels(data=X_train, labels=y_train)\n",
    "y_test = get_mapped_labels(data=X_test, labels=y_test)\n",
    "'''\n",
    "get weights of imbalanced classes\n",
    "'''\n",
    "weights = get_dataset_weight(y_train)\n",
    "'''\n",
    "get train and test loaders\n",
    "'''\n",
    "train_set = get_th_dataset(X_train, y_train)\n",
    "test_set = get_th_dataset(X_test, y_test)\n",
    "train_loader = get_dataloader(train_set, batch_size=batch_size)\n",
    "test_loader = get_dataloader(test_set, batch_size=len(test_set))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training and evaluating"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjding/anaconda3/envs/PLM4ACE/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "model = Cnn(output_dim=1, input_dim=320, drop_out=0, stride=2)\n",
    "optimizer = th.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criteria = ASLSingleLabel(gamma_pos=1, gamma_neg=1, eps = 0.1)  # find the best hyperparameter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjding/anaconda3/envs/PLM4ACE/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/xjding/anaconda3/envs/PLM4ACE/lib/python3.8/site-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "train(model=model, EPOCHS=epochs, optimizer=optimizer, checkpoint=checkpoint, criteria=criteria,\n",
    "      train_set=train_loader, vali_set=test_loader, device=device, LOG_VALIDATION=test_loss_log, LOG_TRAIN=train_loss_log)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xjding/anaconda3/envs/PLM4ACE/lib/python3.8/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": "Cnn(\n  (conv_1): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n  (normalizer_1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (pooling_1): MaxPool1d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (dropout): Dropout(p=0, inplace=False)\n  (fc1): LazyLinear(in_features=0, out_features=64, bias=True)\n  (normalizer_2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (fc2): Linear(in_features=64, out_features=2, bias=True)\n)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = th.load(checkpoint)\n",
    "saved_model = Cnn(output_dim=1, input_dim=320, drop_out=0, stride=2)\n",
    "saved_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "saved_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1552\n",
      "accuracy 1.0\n",
      "precision 1.0\n",
      "f1 1.0\n",
      "recall 1.0\n",
      "matthews_corrcoef 1.0\n",
      "balanced_accuracy 1.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "On train set\n",
    "\"\"\"\n",
    "report(saved_model, train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388\n",
      "accuracy 0.9484536082474226\n",
      "precision 0.9728260869565217\n",
      "f1 0.9470899470899472\n",
      "recall 0.9226804123711341\n",
      "matthews_corrcoef 0.8981011511791757\n",
      "balanced_accuracy 0.9484536082474226\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "On test set\n",
    "\"\"\"\n",
    "report(saved_model, test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
